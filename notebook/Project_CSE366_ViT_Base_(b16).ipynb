{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce067053",
   "metadata": {},
   "source": [
    "# Vit-Base (B/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbda44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Z files\\Project\\CSE366\\CSE366-Group-E-term-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys, math, time, json, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import timm\n",
    "from torchvision import datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, EigenCAM, AblationCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875b6524",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3ce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "img_size = 224\n",
    "epochs = 20\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Anthracnose', 'Canker', 'Dot', 'Healthy', 'Rust']\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = \"../aug-data\"\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), transform=data_transforms[x])\n",
    "    for x in [\"train\", \"val\", \"test\"]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=2)\n",
    "    for x in [\"train\", \"val\", \"test\"]\n",
    "}\n",
    "\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967efff5",
   "metadata": {},
   "source": [
    "## Model: ViT-Base (B/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiah\\AppData\\Local\\Temp\\ipykernel_11008\\191722236.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == \"cuda\"))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_model(num_classes):\n",
    "    model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "    in_features = model.head.in_features\n",
    "    model.head = nn.Sequential(\n",
    "        nn.Dropout(p=0.6),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "num_classes = len(class_names)\n",
    "model = get_model(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cb22f",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best = float('inf')\n",
    "        self.early_stop = False\n",
    "    def step(self, val_loss):\n",
    "        if val_loss < self.best - self.min_delta:\n",
    "            self.best = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5257b5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4962acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_losses, val_losses, val_f1s = [], [], []\n",
    "best_f1 = -1.0\n",
    "best_path = \"../local_saved_model/vit_base_(b/16)_best.pth\"\n",
    "\n",
    "def run_epoch(loader, model, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    pbar = tqdm(loader, total=len(loader))\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True).float()\n",
    "        labels = torch.as_tensor(labels, device=device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(1).detach().cpu().numpy()\n",
    "        y_pred.extend(preds.tolist())\n",
    "        y_true.extend(labels.detach().cpu().numpy().tolist())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    return epoch_loss, f1, report\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    tr_loss, tr_f1, _ = run_epoch(dataloaders[\"train\"], model, train=True)\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_f1, val_report = run_epoch(dataloaders[\"val\"], model, train=False)\n",
    "\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"  train_loss={tr_loss:.4f} | val_loss={val_loss:.4f} | val_f1={val_f1:.4f} | lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\" New best F1: {best_f1:.4f} â€” saved to {best_path}\")\n",
    "\n",
    "    if early_stopping.step(val_loss):\n",
    "        print(\" Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4065b7d",
   "metadata": {},
   "source": [
    "## Plotting Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss (ViT-Base (B/16))\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60fdb3",
   "metadata": {},
   "source": [
    "## Test Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(best_path).exists():\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss, test_f1, test_report = run_epoch(dataloaders[\"test\"], model, train=False)\n",
    "\n",
    "print(\"Test F1:\", test_f1)\n",
    "print(json.dumps(test_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac97ac",
   "metadata": {},
   "source": [
    "## XAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd9de14",
   "metadata": {},
   "source": [
    "### Load model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"vit_base_(b/16)_best.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "idx = random.randint(0, len(dataloaders[\"test\"]) - 1)\n",
    "image, label = dataloaders[\"test\"].dataset[idx]\n",
    "input_tensor = image.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    pred_class = torch.argmax(output, 1).item()\n",
    "\n",
    "print(f\"True Label: {class_names[label]}, Predicted: {class_names[pred_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc4b5f",
   "metadata": {},
   "source": [
    "### Grad-CAM visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262100e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = image.permute(1, 2, 0).numpy()\n",
    "rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "\n",
    "try:\n",
    "    target_layers = [model.stages[-1].blocks[-1].conv_dw]\n",
    "except AttributeError:\n",
    "    target_layers = [model.norm]\n",
    "\n",
    "cams = {\n",
    "    \"Grad-CAM\": GradCAM(\n",
    "        model=model, target_layers=target_layers),\n",
    "    \"Grad-CAM++\": GradCAMPlusPlus(\n",
    "        model=model, target_layers=target_layers),\n",
    "    \"Eigen-CAM\": EigenCAM(\n",
    "        model=model, target_layers=target_layers),\n",
    "    \"Ablation-CAM\": AblationCAM(\n",
    "        model=model, target_layers=target_layers),\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(1, len(cams) + 1, figsize=(20, 5))\n",
    "axs[0].imshow(rgb_img)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "for i, (name, cam_algo) in enumerate(cams.items(), 1):\n",
    "    grayscale_cam = cam_algo(\n",
    "        input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred_class)]\n",
    "    )[0, :]\n",
    "    cam_img = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "    axs[i].imshow(cam_img)\n",
    "    axs[i].set_title(name)\n",
    "    axs[i].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01facb98",
   "metadata": {},
   "source": [
    "### Lime Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(images):\n",
    "    model.eval()\n",
    "    batch = torch.stack([transforms.ToTensor()(img) for img in images], dim=0).to(\n",
    "        device\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(\n",
    "    (rgb_img * 255).astype(np.uint8),\n",
    "    batch_predict,\n",
    "    labels=(pred_class,),\n",
    "    hide_color=0,\n",
    "    num_samples=1000,\n",
    ")\n",
    "\n",
    "lime_img, mask = explanation.get_image_and_mask(\n",
    "    pred_class, positive_only=False, hide_rest=False\n",
    ")\n",
    "plt.imshow(mark_boundaries(lime_img, mask))\n",
    "plt.title(\"LIME\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
