{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed033a8",
   "metadata": {},
   "source": [
    "# Efficintnet b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, time, json, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "from torchvision import datasets, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, EigenCAM, AblationCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eebacd",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "img_size = 224\n",
    "epochs = 20\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de5cc7",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ce16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Anthracnose', 'Canker', 'Dot', 'Healthy', 'Rust']\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = \"../aug-data\"\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), transform=data_transforms[x])\n",
    "    for x in [\"train\", \"val\", \"test\"]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=2)\n",
    "    for x in [\"train\", \"val\", \"test\"]\n",
    "}\n",
    "\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7557c",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best = float('inf')\n",
    "        self.early_stop = False\n",
    "    def step(self, val_loss):\n",
    "        if val_loss < self.best - self.min_delta:\n",
    "            self.best = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26eec8b",
   "metadata": {},
   "source": [
    "## Model Loader: Efficient B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ca92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiah\\AppData\\Local\\Temp\\ipykernel_17156\\3555238829.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler('cuda',enabled=True)\n",
      "c:\\Z files\\Project\\CSE366\\CSE366-Group-E-term-project\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_model(num_classes):\n",
    "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "    model.classifier[1] = nn.Sequential(\n",
    "    nn.Dropout(p=0.6),\n",
    "    nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "num_classes = len(class_names)\n",
    "model = get_model(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7333c1b",
   "metadata": {},
   "source": [
    "## Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7cf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_losses, val_losses, val_f1s = [], [], []\n",
    "best_f1 = -1.0\n",
    "best_path = \"../local_saved_model/efficientnet_b0_best.pth\"\n",
    "\n",
    "def run_epoch(loader, model, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    pbar = tqdm(loader, total=len(loader))\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True).float()\n",
    "        labels = torch.as_tensor(labels, device=device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(1).detach().cpu().numpy()\n",
    "        y_pred.extend(preds.tolist())\n",
    "        y_true.extend(labels.detach().cpu().numpy().tolist())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    return epoch_loss, f1, report\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    tr_loss, tr_f1, _ = run_epoch(dataloaders[\"train\"], model, train=True)\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_f1, val_report = run_epoch(dataloaders[\"val\"], model, train=False)\n",
    "\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"  train_loss={tr_loss:.4f} | val_loss={val_loss:.4f} | val_f1={val_f1:.4f} | lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\" New best F1: {best_f1:.4f} â€” saved to {best_path}\")\n",
    "\n",
    "    if early_stopping.step(val_loss):\n",
    "        print(\" Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189f387",
   "metadata": {},
   "source": [
    "## Plotting Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss (Efficientnet_b0)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741bbdf6",
   "metadata": {},
   "source": [
    "## Test Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(best_path).exists():\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss, test_f1, test_report = run_epoch(dataloaders[\"test\"], model, train=False)\n",
    "\n",
    "print(\"Test F1:\", test_f1)\n",
    "print(json.dumps(test_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7847ef9e",
   "metadata": {},
   "source": [
    "## XAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d883d6f",
   "metadata": {},
   "source": [
    "### Load model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"efficientnet_b0_best.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "idx = random.randint(0, len(dataloaders[\"test\"]) - 1)\n",
    "image, label = dataloaders[\"test\"].dataset[idx]\n",
    "input_tensor = image.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    pred_class = torch.argmax(output, 1).item()\n",
    "\n",
    "print(f\"True Label: {class_names[label]}, Predicted: {class_names[pred_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb8544",
   "metadata": {},
   "source": [
    "### Grad-CAM visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = image.permute(1, 2, 0).numpy()\n",
    "rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "\n",
    "try:\n",
    "    target_layers = [model.features[8]]\n",
    "except AttributeError:\n",
    "    target_layers = [model.features[8]]\n",
    "\n",
    "cams = {\n",
    "    \"Grad-CAM\": GradCAM(\n",
    "        model=model, target_layers=target_layers),\n",
    "    \"Grad-CAM++\": GradCAMPlusPlus(\n",
    "        model=model, target_layers=target_layers),\n",
    "    \"Eigen-CAM\": EigenCAM(\n",
    "        model=model, target_layers=target_layers),\n",
    "    \"Ablation-CAM\": AblationCAM(\n",
    "        model=model, target_layers=target_layers),\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(1, len(cams) + 1, figsize=(20, 5))\n",
    "axs[0].imshow(rgb_img)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "for i, (name, cam_algo) in enumerate(cams.items(), 1):\n",
    "    grayscale_cam = cam_algo(\n",
    "        input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred_class)]\n",
    "    )[0, :]\n",
    "    cam_img = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "    axs[i].imshow(cam_img)\n",
    "    axs[i].set_title(name)\n",
    "    axs[i].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14cef7",
   "metadata": {},
   "source": [
    "### Lime Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(images):\n",
    "    model.eval()\n",
    "    batch = torch.stack([transforms.ToTensor()(img) for img in images], dim=0).to(\n",
    "        device\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(\n",
    "    (rgb_img * 255).astype(np.uint8),\n",
    "    batch_predict,\n",
    "    labels=(pred_class,),\n",
    "    hide_color=0,\n",
    "    num_samples=1000,\n",
    ")\n",
    "\n",
    "lime_img, mask = explanation.get_image_and_mask(\n",
    "    pred_class, positive_only=False, hide_rest=False\n",
    ")\n",
    "plt.imshow(mark_boundaries(lime_img, mask))\n",
    "plt.title(\"LIME\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
