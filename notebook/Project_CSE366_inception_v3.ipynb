{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792f365c",
   "metadata": {},
   "source": [
    "# Inception v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, time, json, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "from torchvision import datasets, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, EigenCAM, AblationCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ccd1ce",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd6166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "img_size = 299\n",
    "epochs = 20\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486060d",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e12496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Anthracnose', 'Canker', 'Dot', 'Healthy', 'Rust']\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = \"../aug-data\"\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), transform=data_transforms[x])\n",
    "    for x in [\"train\", \"val\", \"test\"]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=2)\n",
    "    for x in [\"train\", \"val\", \"test\"]\n",
    "}\n",
    "\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e95f6",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best = float('inf')\n",
    "        self.early_stop = False\n",
    "    def step(self, val_loss):\n",
    "        if val_loss < self.best - self.min_delta:\n",
    "            self.best = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff173aec",
   "metadata": {},
   "source": [
    "## Model Loader: Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310616e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiah\\AppData\\Local\\Temp\\ipykernel_19240\\2380597222.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == \"cuda\"))\n"
     ]
    }
   ],
   "source": [
    "def get_model(num_classes):\n",
    "    model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "    model.AuxLogits.fc = nn.Sequential(\n",
    "        nn.Dropout(p=0.6),\n",
    "        nn.Linear(model.AuxLogits.fc.in_features, num_classes)\n",
    "    )\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=0.6),\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "num_classes = len(class_names)\n",
    "model = get_model(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8983f",
   "metadata": {},
   "source": [
    "## Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_losses, val_losses, val_f1s = [], [], []\n",
    "best_f1 = -1.0\n",
    "best_path = \"../local_saved_model/inception_v3_best.pth\"\n",
    "\n",
    "def run_epoch(loader, model, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    pbar = tqdm(loader, total=len(loader))\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True).float()\n",
    "        labels = torch.as_tensor(labels, device=device)\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs, aux_outputs = model(images)\n",
    "                loss1 = criterion(outputs, labels)\n",
    "                loss2 = criterion(aux_outputs, labels)\n",
    "                loss = loss1 + 0.4 * loss2\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(1).detach().cpu().numpy()\n",
    "        y_pred.extend(preds.tolist())\n",
    "        y_true.extend(labels.detach().cpu().numpy().tolist())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    return epoch_loss, f1, report\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    tr_loss, tr_f1, _ = run_epoch(dataloaders[\"train\"], model, train=True)\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_f1, val_report = run_epoch(dataloaders[\"val\"], model, train=False)\n",
    "\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"  train_loss={tr_loss:.4f} | val_loss={val_loss:.4f} | val_f1={val_f1:.4f} | lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\" New best F1: {best_f1:.4f} â€” saved to {best_path}\")\n",
    "\n",
    "    if early_stopping.step(val_loss):\n",
    "        print(\" Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74fa2a",
   "metadata": {},
   "source": [
    "## Plotting Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss (Inception_v3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23920b46",
   "metadata": {},
   "source": [
    "## Test Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(best_path).exists():\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss, test_f1, test_report = run_epoch(dataloaders[\"test\"], model, train=False)\n",
    "\n",
    "print(\"Test F1:\", test_f1)\n",
    "print(json.dumps(test_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7c8f7",
   "metadata": {},
   "source": [
    "## XAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c3d1e",
   "metadata": {},
   "source": [
    "### Load model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"inception_v3_best.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "idx = random.randint(0, len(dataloaders[\"test\"]) - 1)\n",
    "image, label = dataloaders[\"test\"].dataset[idx]\n",
    "input_tensor = image.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    pred_class = torch.argmax(output, 1).item()\n",
    "\n",
    "print(f\"True Label: {class_names[label]}, Predicted: {class_names[pred_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a2949",
   "metadata": {},
   "source": [
    "### Grad-CAM visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59351b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = image.permute(1, 2, 0).numpy()\n",
    "rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "\n",
    "try:\n",
    "    target_layers = [model.Mixed_7c]\n",
    "except AttributeError:\n",
    "    target_layers = [model.avgpool]\n",
    "\n",
    "cams = {\n",
    "    \"Grad-CAM\": GradCAM(\n",
    "        model=model, target_layers=target_layers),\n",
    "    \"Grad-CAM++\": GradCAMPlusPlus(\n",
    "        model=model, target_layers=target_layers),\n",
    "    \"Eigen-CAM\": EigenCAM(\n",
    "        model=model, target_layers=target_layers),\n",
    "    \"Ablation-CAM\": AblationCAM(\n",
    "        model=model, target_layers=target_layers),\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(1, len(cams) + 1, figsize=(20, 5))\n",
    "axs[0].imshow(rgb_img)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "for i, (name, cam_algo) in enumerate(cams.items(), 1):\n",
    "    grayscale_cam = cam_algo(\n",
    "        input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred_class)]\n",
    "    )[0, :]\n",
    "    cam_img = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "    axs[i].imshow(cam_img)\n",
    "    axs[i].set_title(name)\n",
    "    axs[i].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f6bd0",
   "metadata": {},
   "source": [
    "### Lime Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(images):\n",
    "    model.eval()\n",
    "    batch = torch.stack([transforms.ToTensor()(img) for img in images], dim=0).to(\n",
    "        device\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(\n",
    "    (rgb_img * 255).astype(np.uint8),\n",
    "    batch_predict,\n",
    "    labels=(pred_class,),\n",
    "    hide_color=0,\n",
    "    num_samples=1000,\n",
    ")\n",
    "\n",
    "lime_img, mask = explanation.get_image_and_mask(\n",
    "    pred_class, positive_only=False, hide_rest=False\n",
    ")\n",
    "plt.imshow(mark_boundaries(lime_img, mask))\n",
    "plt.title(\"LIME\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
